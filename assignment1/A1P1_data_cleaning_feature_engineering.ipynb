{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AIAP Banner](../images/AIAP-Banner.png \"AIAP Banner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 1 - Part 1: \n",
    "<br>\n",
    "Data Cleaning & Feature Engineering</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Name: Wong Khee Ern</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting...\n",
    "\n",
    "### How to structure your answers\n",
    "\n",
    "There are many different ways to approach a problem in AI/ML. When attempting the problems in the notebooks, keep in mind that there is no definite \"correct way\" to handle any problem. Instead, when choosing which method to use, focus on how current literature approaches the problem, how you intend to evaluate the effectiveness of the solution you are proposing and how to improve the solution if necessary. Similarly, for the \"open-ended\" questions, it is important to substantiate your answers with supporting reasons. Be sure to bounce your ideas off each other to see how different people approach the same problem!\n",
    "\n",
    "### Coding conventions\n",
    "\n",
    "Learning good habits and ensuring your code follows a certain convention is very important in an environment where your code will be shared and read by others. Following the standard conventions highlighted in the [PEP-8 document](https://www.python.org/dev/peps/pep-0008/) is a good start.\n",
    "\n",
    "### Reproducible Data Pipeline\n",
    "\n",
    "In the next few sections, you will be creating a data pipeline in a step by step process. At the end of this notebook, you have to combine all of these steps into a Python module named `datapipeline.py` in the [src folder](./src). The module should contain at least a function with the following signature:\n",
    "\n",
    "```python\n",
    "def transform(data_path):\n",
    "  \"\"\"\n",
    "  :param data_path: ......\n",
    "  :return: ......\n",
    "  \"\"\"\n",
    "  return feature_engineered_dataframe\n",
    "```\n",
    "\n",
    "Once complete, you will be able to use this function freely in this notebook or other notebooks. This will ensure consistency in the data transformation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Most machine learning projects follow a typical flow. \n",
    "\n",
    "    - Defining the problem statement\n",
    "    - Identifying an appropriate dataset\n",
    "    - Extracting the relevant data from data sources\n",
    "    - Defining and labelling the dataset\n",
    "    - Data cleaning\n",
    "    - Exploring the dataset, refered to as Exploratory Data Analysis (EDA)\n",
    "    - Feature engineering\n",
    "    - Selecting and training appropriate models\n",
    "    - Evaluating the model\n",
    "    - Repeating the above steps (data identification to model evaluation) until desired performance is achieved\n",
    "    - Deploying the model\n",
    "    - Model maintenance and retraining (if necessary)\n",
    "\n",
    "\n",
    "For this assignment, we will explore clustering in unsupervised learning. Keep this in mind when performing the data preparation steps (data extraction, data cleaning, EDA and feature engineering) in the next sections of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Topics\n",
    "1. SQL query\n",
    "2. Data cleaning\n",
    "3. Exploratory data analysis\n",
    "4. Feature Engineering\n",
    "5. Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Deliverables\n",
    "1. Jupyter notebook\n",
    "2. Script: `datapipeline.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Extraction\n",
    "\n",
    "The data for this assignment were modified from the US census-income dataset on the UCI dataset repository. **Please do not download the data directly from the UCI repository. Instead, follow the instructions below to query the dataset from our database.** Additionally, please refer to this [link](https://www2.1010data.com/documentationcenter/beta/Tutorials/MachineLearningExamples/CensusIncomeDataSet.html) for a description of the variables.\n",
    "\n",
    "The data are stored in a database. If you are not familiar with databases, please refer to this [link](https://medium.com/@rwilliams_bv/intro-to-databases-for-people-who-dont-know-a-whole-lot-about-them-a64ae9af712) to obtain a high level overview of databases and their related terms.\n",
    "\n",
    "The type of database used is an Azure SQL Server instance. SQL Server is Microsoft's RDBMS product offering which uses a variant of SQL (Structured Query Language) called T-SQL (Microsoft Transact-SQL).\n",
    "There are many resources available online to learn how to write T-SQL and you should be able to find one that fits to your level of understanding. One such resource is on [TutorialsPoint](https://www.tutorialspoint.com/t_sql/index.htm). In addition, you can use Microsoft's [reference pages](https://docs.microsoft.com/en-us/sql/t-sql/language-reference?view=sql-server-ver15) to quickly look up syntax documentation.\n",
    "At a minimum, you should be able to combine and extract data from multiple tables in an efficient manner so that you can complete your assignments.\n",
    "\n",
    "If you have trouble accessing the database, you may have to download the Microsoft ODBC Driver. Follow the download instructions for [Windows](https://docs.microsoft.com/en-us/sql/connect/odbc/windows/system-requirements-installation-and-driver-files?view=sql-server-ver15) or [Mac/Linux](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver15#microsoft-odbc-driver-131-for-sql-server).\n",
    "\n",
    "The data is hosted on an Azure SQL Server with the following details:\n",
    "\n",
    "    server = 'aiap-training.database.windows.net'\n",
    "    database = 'aiap'\n",
    "    username = 'apprentice'\n",
    "    password = 'Pa55w.rd'\n",
    "    driver= '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "\n",
    "There are 3 separate tables, `basic_bio`, `employment`, `residential_tax`. All of these tables have an `id` column that can be used to merge them.\n",
    "\n",
    "\n",
    "The pandas package has a `read_sql_query` function that can be used to access the data. You may require another python package to use this function.\n",
    "\n",
    "#### 2.1. Query all 3 tables from the SQL server and combine them into a single pandas dataframe. Save this dataframe as a `.csv` file on your local computer with the filepath `assignment1/data/raw/data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection details\n",
    "server = 'aiap-training.database.windows.net'\n",
    "database = 'aiap'\n",
    "username = 'apprentice'\n",
    "password = 'Pa55w.rd'\n",
    "\n",
    "# URL encode the password\n",
    "password_encoded = urllib.parse.quote_plus(password)\n",
    "\n",
    "# Create SQLAlchemy URL format connection string\n",
    "connection_string = f\"mssql+pyodbc://{username}:{password_encoded}@{server}/{database}?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes&Connection+Timeout=60&Encrypt=yes\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Query all 3 tables\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM basic_bio b\n",
    "INNER JOIN employment e ON b.id = e.id\n",
    "INNER JOIN residential_tax r ON b.id = r.id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File saved: assignment1/data/raw/data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the data (Jupyter uses current dir as the 'root')\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "df.to_csv('data/raw/data.csv', index=False)\n",
    "\n",
    "print(\"✅ File saved: assignment1/data/raw/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "Whenever we have a new dataset, we need to inspect and clean the dataset.\n",
    "\n",
    "Some (non-exhaustive) steps in data cleaning are:\n",
    "    - handling missing values\n",
    "    - checking for irrelevant rows and/or columns\n",
    "    - checking for duplicate rows and deciding whether to drop them\n",
    "    \n",
    "    \n",
    "#### 3.1. List the steps you intend to take to clean the data in the markdown cell below. Give a brief explanation for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data\n",
    "1. Find out the shape of the data\n",
    "2. Find out what the columns mean and its data type\n",
    "3. Inspect each column individually for: (use plots to visualize if needed)\n",
    "    a. Missing values (msno, )\n",
    "    b. Outliers (describe(), IQR, box plots)\n",
    "    c. Distribution (histogram, countplots etc.)\n",
    "4. Check for duplicates\n",
    "5. Check for correlation, effect sizes between different columns (corr matrix, heatmap)\n",
    "\n",
    "Clean data\n",
    "1. For each column:\n",
    "    a. Missing values\n",
    "        i. Check if missing values are correlated with other columns\n",
    "        ii. Decide on the best way to fill in missing values\n",
    "            - Ideally, try to find patterns in data to fill in missing values\n",
    "            - Else, try mean, mode, median imputation\n",
    "            - If too many missing values, can consider dropping the col\n",
    "    b. Outliers\n",
    "        i. Find out more domain knowledge to see if outliers are valid or simply data errors\n",
    "        ii. Analyze their potential impact on the ML model\n",
    "        iii. Identify methods to address outliers (log, winsorizing etc, knn etc.)\n",
    "    c. Others\n",
    "        i. Spelling errors, data type and formatting inconsistencies, formatting date time, rules violation (e.g. negative age)\n",
    "2. Drop duplicate data where necessary by applying domain knowledge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Write each data cleaning step as its own function so that they can be reused individually. Ideally, you should organise the functions so that they can be put into their own library. Remember to follow the [PEP8](https://www.python.org/dev/peps/pep-0008/) convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'id': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'age': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'education': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'enroll_in_edu_inst_last_wk': type = <class 'pandas.core.series.Series'>\n",
      "Column 'marital_stat': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'race': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'sex': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'country_of_birth_father': type = <class 'pandas.core.series.Series'>\n",
      "Column 'country_of_birth_mother': type = <class 'pandas.core.series.Series'>\n",
      "Column 'country_of_birth_self': type = <class 'pandas.core.series.Series'>\n",
      "Column 'citizenship': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'id': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'age': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'class_of_worker': type = <class 'pandas.core.series.Series'>\n",
      "Column 'detailed_industry_recode': type = <class 'pandas.core.series.Series'>\n",
      "Column 'detailed_occupation_recode': type = <class 'pandas.core.series.Series'>\n",
      "Column 'education': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'wage_per_hour': type = <class 'pandas.core.series.Series'>\n",
      "Column 'marital_stat': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'major_industry_code': type = <class 'pandas.core.series.Series'>\n",
      "Column 'major_occupation_code': type = <class 'pandas.core.series.Series'>\n",
      "Column 'race': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'hispanic_origin': type = <class 'pandas.core.series.Series'>\n",
      "Column 'sex': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'member_of_a_labor_union': type = <class 'pandas.core.series.Series'>\n",
      "Column 'reason_for_unemployment': type = <class 'pandas.core.series.Series'>\n",
      "Column 'full_or_part_time_employment_stat': type = <class 'pandas.core.series.Series'>\n",
      "Column 'detailed_household_and_family_stat': type = <class 'pandas.core.series.Series'>\n",
      "Column 'detailed_household_summary_in_household': type = <class 'pandas.core.series.Series'>\n",
      "Column 'num_persons_worked_for_employer': type = <class 'pandas.core.series.Series'>\n",
      "Column 'family_members_under_18': type = <class 'pandas.core.series.Series'>\n",
      "Column 'citizenship': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'own_business_or_self_employed': type = <class 'pandas.core.series.Series'>\n",
      "Column 'weeks_worked_in_year': type = <class 'pandas.core.series.Series'>\n",
      "Column 'year': type = <class 'pandas.core.series.Series'>\n",
      "Column 'id': type = <class 'pandas.core.frame.DataFrame'>\n",
      "Column 'capital_gains': type = <class 'pandas.core.series.Series'>\n",
      "Column 'capital_losses': type = <class 'pandas.core.series.Series'>\n",
      "Column 'dividends_from_stocks': type = <class 'pandas.core.series.Series'>\n",
      "Column 'tax_filer_stat': type = <class 'pandas.core.series.Series'>\n",
      "Column 'region_of_previous_residence': type = <class 'pandas.core.series.Series'>\n",
      "Column 'state_of_previous_residence': type = <class 'pandas.core.series.Series'>\n",
      "Column 'migration_code_change_in_msa': type = <class 'pandas.core.series.Series'>\n",
      "Column 'migration_code_change_in_reg': type = <class 'pandas.core.series.Series'>\n",
      "Column 'fill_inc_questionnaire_for_veteran_s_admin': type = <class 'pandas.core.series.Series'>\n",
      "Column 'veterans_benefits': type = <class 'pandas.core.series.Series'>\n",
      "Column 'income_group': type = <class 'pandas.core.series.Series'>\n",
      "Column 'migration_code_move_within_reg': type = <class 'pandas.core.series.Series'>\n",
      "Column 'live_in_this_house_1_year_ago': type = <class 'pandas.core.series.Series'>\n",
      "Column 'migration_prev_res_in_sunbelt': type = <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:  \n",
    "    print(f\"Column '{col}': type = {type(df[col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199463, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA OVERVIEW: Dataset\n",
      "============================================================\n",
      "Shape: 199,463 rows × 50 columns\n",
      "Memory Usage: 436.10 MB\n",
      "\n",
      "Data Types:\n",
      "  object      : 34 columns\n",
      "  int64       : 16 columns\n",
      "\n",
      "Column                    Type            Non-Null   Null%    Unique  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m/var/folders/ml/_76bxy6567s7g1bj22wznv_h0000gn/T/ipykernel_30754/3433182202.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m        \u001b[33m'potential_id_columns'\u001b[39m: potential_ids\n\u001b[32m     96\u001b[39m    }\n\u001b[32m     97\u001b[39m \n\u001b[32m     98\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m data_overview(df)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# stats = data_overview(df, name=\"Sales Dataset\")\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# print(f\"Dataset has {stats['duplicate_rows']} duplicate rows\")\u001b[39;00m\n",
      "\u001b[32m/var/folders/ml/_76bxy6567s7g1bj22wznv_h0000gn/T/ipykernel_30754/3433182202.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, name)\u001b[39m\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m    overview_stats = {}\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m    \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;28;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m        col_type = str(df[col].dtype)\n\u001b[32m     45\u001b[39m        non_null_count = df[col].count()\n\u001b[32m     46\u001b[39m        null_pct = ((n_rows - non_null_count) / n_rows * \u001b[32m100\u001b[39m)\n\u001b[32m     47\u001b[39m        unique_count = df[col].nunique()\n",
      "\u001b[32m/opt/anaconda3/envs/data-science/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6315\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6316\u001b[39m         ):\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6318\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "def data_overview(df, name=\"Dataset\"):\n",
    "   \"\"\"\n",
    "   Comprehensive data overview for exploratory data analysis.\n",
    "   \n",
    "   Parameters:\n",
    "   -----------\n",
    "   df : pandas.DataFrame\n",
    "       The dataset to analyze\n",
    "   name : str, optional\n",
    "       Name of the dataset for display purposes (default: \"Dataset\")\n",
    "   \n",
    "   Returns:\n",
    "   --------\n",
    "   dict\n",
    "       Dictionary containing overview statistics for programmatic use\n",
    "   \"\"\"\n",
    "   \n",
    "   print(f\"{'='*60}\")\n",
    "   print(f\"DATA OVERVIEW: {name}\")\n",
    "   print(f\"{'='*60}\")\n",
    "   \n",
    "   # Basic shape information\n",
    "   n_rows, n_cols = df.shape\n",
    "   print(f\"Shape: {n_rows:,} rows × {n_cols} columns\")\n",
    "   \n",
    "   # Memory usage\n",
    "   memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "   print(f\"Memory Usage: {memory_mb:.2f} MB\")\n",
    "   \n",
    "   # Data types breakdown\n",
    "   print(\"\\nData Types:\")\n",
    "   dtype_counts = df.dtypes.value_counts()\n",
    "   for dtype, count in dtype_counts.items():\n",
    "       print(f\"  {str(dtype):<12}: {count} columns\")\n",
    "   \n",
    "   # Column overview\n",
    "   print()\n",
    "   print(f\"{'Column':<25} {'Type':<15} {'Non-Null':<10} {'Null%':<8} {'Unique':<8}\")\n",
    "   print(\"-\" * 70)\n",
    "   \n",
    "   overview_stats = {}\n",
    "   \n",
    "   for col in df.columns:\n",
    "       col_type = str(df[col].dtype)\n",
    "       non_null_count = df[col].count()\n",
    "       null_pct = ((n_rows - non_null_count) / n_rows * 100)\n",
    "       unique_count = df[col].nunique()\n",
    "       \n",
    "       print(f\"{col[:24]:<25} {col_type:<15} {non_null_count:<10} \"\n",
    "             f\"{null_pct:>6.1f}% {unique_count:<8}\")\n",
    "       \n",
    "       # Store for programmatic use\n",
    "       overview_stats[col] = {\n",
    "           'dtype': col_type,\n",
    "           'non_null_count': non_null_count,\n",
    "           'null_percentage': null_pct,\n",
    "           'unique_count': unique_count\n",
    "       }\n",
    "   \n",
    "   # Summary statistics\n",
    "   missing_cols = df.isnull().sum()\n",
    "   high_missing = missing_cols[missing_cols > n_rows * 0.5]\n",
    "   \n",
    "   print(f\"\\n{'='*30}\")\n",
    "   print(\"SUMMARY\")\n",
    "   print(f\"{'='*30}\")\n",
    "   print(f\"Total missing values: {df.isnull().sum().sum():,}\")\n",
    "   print(f\"Columns with >50% missing: {len(high_missing)}\")\n",
    "   \n",
    "   if len(high_missing) > 0:\n",
    "       print(\"High missing columns:\", list(high_missing.index))\n",
    "   \n",
    "   # Potential issues\n",
    "   print(\"\\nPOTENTIAL ISSUES:\")\n",
    "   duplicate_rows = df.duplicated().sum()\n",
    "   print(\"• Duplicate rows: {duplicate_rows:,}\")\n",
    "   \n",
    "   # Check for columns that might be IDs\n",
    "   potential_ids = []\n",
    "   for col in df.columns:\n",
    "       if df[col].nunique() == n_rows and n_rows > 1:\n",
    "           potential_ids.append(col)\n",
    "   \n",
    "   if potential_ids:\n",
    "       print(f\"• Potential ID columns: {potential_ids}\")\n",
    "   \n",
    "   # Return summary for programmatic use\n",
    "   return {\n",
    "       'shape': (n_rows, n_cols),\n",
    "       'memory_mb': memory_mb,\n",
    "       'dtype_counts': dtype_counts.to_dict(),\n",
    "       'column_stats': overview_stats,\n",
    "       'total_missing': df.isnull().sum().sum(),\n",
    "       'duplicate_rows': duplicate_rows,\n",
    "       'potential_id_columns': potential_ids\n",
    "   }\n",
    "\n",
    "\n",
    "data_overview(df)\n",
    "# Example usage:\n",
    "# stats = data_overview(df, name=\"Sales Dataset\")\n",
    "# print(f\"Dataset has {stats['duplicate_rows']} duplicate rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "After the data has been cleaned, we now need to perform EDA to get a better understanding of the dataset. As a start, you can consider looking at the distributions of each variable as well as the correlation between each pair of variables. Please explore further if you find any meaningful insights. You can refer to this [book chapter](https://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf) for detailed information on EDA.\n",
    "\n",
    "\n",
    "#### 4.1. Perform EDA on the dataset. Include all figures / statistics you use. Briefly describe the purpose for each EDA step and the finding(s) from each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "\n",
    "Feature engineering is the process of transforming variables so that they are potentially more useful for the task at hand. Domain expertise is extremely useful for feature engineering. Hence, it is always important to try and understand as much as you can about the problem's domain.\n",
    "\n",
    "Apart from using domain knowledge, we can also perform some simple feature engineering by aggregating different categories within a variable.\n",
    "\n",
    "When conducting feature engineering, it is important to keep in mind the task at hand and take reference from the insights that were derived from the EDA. This will ensure that the features that were created will be useful for the task at hand. The main task that you are feature engineering for is unsupervised learning (clustering).\n",
    "\n",
    "#### 5.1. Engineer 3 new features for this dataset. Briefly explain why you chose these features and how you think they could be useful for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reproducible Data Pipeline\n",
    "\n",
    "#### 6.1. Finally, take all the code you've written and create a Python module named `datapipeline.py` in the [src folder](./src). The module should at least contain the following function. The transform function should take in the absolute path to the data file as a parameter and return a pandas dataframe.\n",
    "\n",
    "```python\n",
    "def transform(data_path):\n",
    "  \"\"\"\n",
    "  :param data_path: ......\n",
    "  :return: ......\n",
    "  \"\"\"\n",
    "  return feature_engineered_dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission\n",
    "\n",
    "\n",
    "#### 7.1. Create a `conda.yml` file at the base of the assignment folder. Add (manually) your required dependencies to the file named `conda.yml` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Clustering Algorithms\n",
    "\n",
    "Load the '[`A1P2_clustering.ipynb`](A1P2_clustering.ipynb)' notebook and start working from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>End of Assignment 1 - Part 1: Data Cleaning & Feature Engineering</center></h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
